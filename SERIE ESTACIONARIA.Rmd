---
title: "Trabajo Práctico Final - Estadística Actuarial (751-1)"  

author:  

  - Zajdenberg, Dana Rebeca (892.786)
  - Barrales Agosti, Julián Guido (889.901)
  - Cohen Falah, Iván Nahuel (890.693)
  - Lopatka, Lucas Eitán (889.524)
  - Borovsky, Ignacio Ariel(891.119)
indent: yes
output:
  pdf_document: default
  word_document: default
  html_document: default
geometry: margin=2cm
---
  
# Introducción  
\  
\  
\ En el presente trabajo se analizarán dos series temporales correspondientes a dos activos financieros distintos, otorgadas por el cuerpo docente. A lo largo del mismo se empleará la metodología de Box-Jenkins, utilizando los conceptos vistos en clase y el programa R-Studio.

# Marco teórico  
\  
\  
\ Se define un proceso estocástico como una familia de variables aleatorias que corresponden a momentos sucesivos del tiempo. Será designado por $Y_(t,u)$ donde $t$ es el tiempo y $u$ es la variable aleatoria.  
  
\ En el siguiente trabajo, se trabajará con una clase especial de los procesos estocásticos: los procesos lineales. Estos procesos se caracterizan porque se pueden representar como una combinación lineal de variables aleatorias.   

\ Son entonces procesos estocasticos lineales:  

* El proceso puramente aleatorio $Y_t$=$\epsilon_t$ donde $\epsilon_t$ satisface las siguientes propiedades: 
$E[\epsilon_t]=0$, $E[\epsilon_t]^2=\sigma^2$ y $E[\epsilon_t,\epsilon_t]=0$
En el tratamiento de series temporales, se suele designar a este proceso con la denominacion de "ruido blanco". De ahora en adelante, por $\epsilon_t$ se designa únicamente a una variable aleatoria que goza de dichas propiedades

* El proceso autoregresivo de orden *p* AR(P) se expresa de la siguiente forma, $$y_t=\phi_1y_{t-1}+\phi_2y_{t-2}+...+\phi_py_{t-p}$$
Al ser p el lag máximo que aparece en el proceso, la denominación de autorregresivo procede de que $y_t$ se obtiene mediante una regresión sobre valores desfasados de la propia variable.

* El proceso de medias móviles de orden *q* MA(q) dado por, 
$$y_t=\epsilon_t-\theta_1\epsilon_{t-1}-\theta_2\epsilon_{t-2}-...-\theta_q\epsilon_{t-q}$$
hace referencia a que la variable $y_t$ se obtiene como un promedio de variables de "ruido blanco", donde las $\theta$ resultan ser las variables de ponderación.

* Mediante una combinación de un proceso autorregresivo y un proceso demedias moviles se obtiene un proceso ARMA(p,q), donde *p* indica el lag maximo de la parte autorregresiva y *q* señala el correspndiente a la parte de las medias moviles. La expresión de un modelo ARMA es la siguiente: 
$$y_t=\phi_1y_{t-1}+...+\phi_py_{t-p}+\epsilon_t-\theta_1\epsilon_{t-1}-...-\theta_q\epsilon_{t-q}$$

\ En estos procesos estocásticos se supone que cumplen con las condiciones de estacionariedad e invertibilidad. Si se levanta este supuesto el modelo se encuentra en el campo de la no estacionariedad.
\ Para verificar este supuesto se debe cumplir que las raíces del polinomio caracteristico sean mayores a la unidad. 
  
\ Sea un proceso AR(1) con $\phi_1=1$ (es decir, un random walk), se toman diferencias de primer orden para pasar de un proceso $y_t$ a uno $w_t$

* Todos aquellos modelos que se pueden transformar en estacionarios mediante la toma de diferencias de un determinado orden son los denominados modelos ARIMA(p,d,q)  

#### Metodologia de elaboracion de modelos ARIMA  
\  
\ Hasta ahora, se ha adoptado la perspectiva de un proceso conocido previamente, a partir del cual se han generado una o más realizaciones. El objetivo es, a partir de los valores de una serie temporal, llegar a un conocimiento del mecanismo que verosimilmente haya podido generar la serie.  
  
\ Para tal fin, se utilizará la el procedimiento que proponen Box y Jenkins. La misma consiste en varias etapas:

1. Fase de identificación
2. Fase de estimación
3. Fase de validación
4. Fase de predicción  

\ Para la **fase de identificación** se identificará el orden de diferenciación. Si se observa la función de autocorrelación, en el caso de que el proceso sea estacionario, decrecerá rapidamente o de manera sinusoidal. Si se cumple el supuesto de que la raíz se aproxima a la unidad, la FAC decrecerá lentamente, lo cual indica que se trata de un proceso no estacionario.  
  
\ Una vez diferenciada (o no) la serie, se observa la apariencia general de la función de autocorrelación y función de autocorrelación parcial para obtener pistas sobre la elección de los órdenes *p* y *q*.

|             	| FACT                                                                                                                                           	| FACPT                                                                                                                                           	|
|:-----------:	|------------------------------------------------------------------------------------------------------------------------------------------------	|-------------------------------------------------------------------------------------------------------------------------------------------------	|
| AR(p)       	| Decrecimiento rápido de tipo geométrico puro, y geométrico con alteración de signos, sinusoidal o mezcla de varios tipos.                      	| Se anula para lags superiores a "p"                                                                                                             	|
| MA(q)       	| Se anula para lags superiores a "q"                                                                                                            	| Decrecimiento rápido de tipo exponencial y/o sinusoidal                                                                                         	|
|  ARMA (p,q) 	| Los primeros valores iniciales no tienen patrón fijo y van seguidos de una mezcla de oscilaciones sinusoidales y/o exponenciales amortiguadas. 	|  Los primeros valores iniciales no tienen patrón fijo y van seguidos de una mezcla de oscilaciones sinusoidales y/o exponenciales amortiguadas. 	|  
  
\ Ahora bien, el comportamiento de la FAC y la FACP da una intuición subjetiva del orden de integración del modelo. Con la finalidad de decidir si el proceso $y_t$ puede ser considerado o no estacionario se utilizará un test de raíz unitaria, particularmente el test de Dickey y Fuller.  
  
\ La ecuación de Dickey y Fuller es $y_t-y_{t-1}=(\phi_1-1)y_{t-1}+\epsilon_t$ donde se testea la hipotesis nula de que $\phi_1-1=0$ que podria ser contrastada respecto de la alternativa $\phi_1-1<0$, se utiliza el estdístico Tau. Dickey y Fuller demostraron que la definicion de la zona crítica depende, ademas del tamaño de la muestra, de la forma de la ecuación autorregresiva (por ejemplo, si incluye o no término independente o términos de tendencia determinístca) y definieron tres clases de valores críticos para ser utilizados en los casos que:

+ $\tau$: no incluya término independiente ni término lineal
+ $\tau_\mu$: incluya término independiente, pero no incluya término lineal
+ $\tau_\tau$: incluya ambos términos

\ Dado que el estimador de $\phi_1$ es positivo se trata de un test de una cola, se considerará que existen razones suficientes para rechazar la hipótesis nula si el valor del estadístico $\tau=\frac{\hat\phi_1-1}{\hat\sigma(\hat\phi_1)}$ es menor que el correspondiente valor $\tau$  
\  
\  Luego de identificar la estacionariedad (o no) de la serie se procede a la segunda parte de esta fase. Se debe identificar el orden de la parte autorregresiva (p) y de medias moviles (q).
\  La determinación mas apropiada del orden del modelo se puede lograr con el uso de criterios objetivos de selección de modelos tales como Akaike (AIC), Schwarz (BIC) o Hannan-Quinn (HQ). Este análisis procede con BIC como criterio seleccionado.  
\    
\  La **fase de estimación** consiste entonces en obtener los estimadores de los parámetros del modelo identificado. Existen diversos métodos de estimación para modelos ARMA como el método de mínimos cuadrados y el método de máxima verosimilitud. Se utilizará el primero.    
\    
\  Una vez que se dispone del modelo identificado y estimado, se procede a **fase de validación**. 

#### Análisis de los residuos
\  Si el modelo especificado es el correcto, y se conocierna los parametros verdaderos y los valores iniciales, se obtiene una serie temporal de "ruido blanco". El analisis deben hacerse entonces a partir de los residuos. Si el comportamiento de los residuos se asemeja al de una serie ruido blanco, existirá una ecuación entre el modelo identificado y los datos.  
\  Con el objetivo de testear si efectivamente los residuos del modelo se asemejan a un ruido blanco se ejecutan dos test:  

* *Test de Incorrelación de Ljung-box*: Pone a prueba la incorrelación de los residuos, es decir, a la hipotesis nula de que $\rho_1=\rho_2=...=\rho_M=0$. A medida que aumenta M disminuye la potencia del contraste.  
\  Ljung y Box propusieron el siguiente estadístico:

$$Q=T(T+2)\sum^M_{t=1}(T-t)^{-1}r^2_t$$
se distribuye como una $\chi^2$ con $M-p-q$ grados de libertad.

* *Test de normalidad de Jarque-Bera*: Pone a prueba la normalidad de los residuos. Bajo la hipótesis nula $\epsilon_t~Normal()$ contra una alternativa no específica. El estadístico de Jarque-Bera es

$$JB=\frac{n}{4}(S^2+\frac{(C-3)^2}{4})$$

donde *S* es el coeficiente de asimetría de los residuos y C el coeficiente de curtosis. El estadístico JB se distribuye como una $\chi^2$ con *2* grados de libertad.

#### Analisis de los coeficientes estimados

\  Se contrasta la significatividad de los parámetros del modelo. El estadístico *t* esta construido sobre la hipotesis nula de que el parámetro es igual a 0. Entonces, para un coeficiente - $\phi_1$ por ejemplo - el estadístico *t* viene dado por la siguiente expresión:

$$t_{N-p-q-\delta}=\frac{\hat\phi_1-(\phi_1/H_0)}{\hat\sigma_\phi}$$

\  Los modelos ARIMA se aplican en general a muestras grandes, por lo que se pueden utilizar los niveles de significación de la normal. Asi, a *groso modo* se rechazaría la hipotesis nula de que $\phi_1=0$, para un nivel de significación del 5% cuando $|t^*|>=2$

#### Bondad de ajuste

\ El criterio de Akaike consiste en seleccionar aquel modelo para el que se obtenga un estadístico AIC mas bajo. Este estadístico no presenta el inconveniente que presenta el $R^2$ y el $R^2$ corregido, pues penaliza los modelos con mayor número de parámetros y además permite comparar modelos con transformaciones de Box-Cox diferentes.  

#### Reformulación del modelo

\ Si después de aplicar los contrastes y análisis de los epigráfes anteriores se llega a la conclusión de que el modelo seleccionado no es adecuado, se debe  reformular el modelo.    
\    
\  Una vez que se seleccionó y validó el modelo, la **fase de predicción** consiste en en utiilzar este modelo en la predicción de valores futuros de la variable objeto de estudio. Dado un proceso ARMA $\phi(L)Y_t=\theta(L)\epsilon_t$ el valor que se trata predecir se expresa para el valor de $Y_{T+l}$ de la siguiente forma:

$$Y_{T+l}=\phi_1Y_{T+l-1}+...+\phi_pY_{T+l-p}-\epsilon_{T+l}-\theta_{1}\epsilon_{T+l-1}-...-\theta_q\epsilon_{T+l-q}$$ 
\ Para los modelos de medias moviles se cumple que el predictor se anula cuando $l>q$. En general, para el cálculo de predicciones de valores futuros se utiliza el modelo en la forma original ARMA, donde la parte AR esta constituída por coeficientes autorregresivos generalizados.

```{r, echo=FALSE, message=FALSE}
library(moments)
library(tseries)
library(urca)
library(forecast)
library(nortest)
library(kableExtra)
```

```{r, echo=FALSE}
Serie1 <- read.csv2("/Users/iborovsky/Desktop/TPR/Grupo 1 - activo financiero.csv")

SerieA<-ts(Serie1[,1])
SerieB<-ts(Serie1[,2])

minimoA<-c(min(SerieA))
maximoA<-c(max(SerieA))
percentil1A<-c(quantile(SerieA,0.25))
medianaA<-c(median(SerieA))
percentil3A<-c(quantile(SerieA,0.75))
asimetriaA<-c(skewness(SerieA))
kurtosisA<-c(kurtosis(SerieA))
promedioA<-c(mean(SerieA))
varianzaA<-c(var(SerieA))
desvioA<-c(sqrt(var(SerieA)))

minimoB<-c(min(SerieB))
maximoB<-c(max(SerieB))
percentil1B<-c(quantile(SerieB,0.25))
medianaB<-c(median(SerieB))
percentil3B<-c(quantile(SerieB,0.75))
asimetriaB<-c(skewness(SerieB))
kurtosisB<-c(kurtosis(SerieB))
promedioB<-c(mean(SerieB))
varianzaB<-c(var(SerieB))
desvioB<-c(sqrt(var(SerieB)))

matrizAnalisisDescriptivo<-matrix(c(minimoA,maximoA,percentil1A,medianaA,percentil3A,asimetriaA,kurtosisA, promedioA, varianzaA, desvioA), c(minimoB,maximoB,percentil1B,medianaB,percentil3B,asimetriaB,kurtosisB, promedioB, varianzaB, desvioB), nrow = 10, ncol = 2,dimnames = list(c("Minimo","Maximo","Percentil 0.25","Mediana","Percentil 0.75","Coef Asimetria","Coef Curtosis", "Promedio", "Varianza", "Desvio"), c("Serie A", "Serie B")))
knitr::kable(matrizAnalisisDescriptivo)

```

#### Fase de identificación

\ Se grafican la series y sus funciones de autocovarianza y autocorrelacion para una primera impresión
  
```{r, echo=FALSE, fig.width=20, fig.height=5}
par(mfrow=c(1,2))
boxplot(SerieA, main= "Grafico de BoxPlot Serie A", horizontal = T)
boxplot(SerieB, main= "Grafico de BoxPlot Serie B", horizontal = T)
```

```{r, echo=FALSE, fig.width=20, fig.height=10}
par(mfrow=c(2,3))
acf(SerieA,main="FAS Serie A", type="covariance")
acf(SerieA,main="FAC Serie A")
pacf(SerieA,main="FACP Serie A")
acf(SerieB,main="FAS Serie B", type="covariance")
acf(SerieB,main="FAC Serie B")
pacf(SerieB,main="FACP Serie B")
```

\ Para la serie A, el gráfico de la FAC muestra un decrecimiento sinusoidal. En cuanto a la serie B, a través de la FACP se verifica que decrece lentamente de manera sinusoidal. Los gráficos proponen la posibilidad de que la serie B sea un proceso de medias moviles no estacionario. Para confirmar estas intuiciones son realizados los correspondientes tests de Dickey-Füller con el objetivo de determinar la existencia o no de raíces unitarias.

```{r, echo=FALSE}
trend.dfA<-ur.df(SerieA,type="trend",lags=4)
drift.dfA<-ur.df(SerieA,type="drift",lags=4)
none.dfA<-ur.df(SerieA,type="none",lags=4)
trend.dfB<-ur.df(SerieB,type="trend",lags=4)
drift.dfB<-ur.df(SerieB,type="drift",lags=4)
none.dfB<-ur.df(SerieB,type="none",lags=4)
```

#### Los valores de Tau empíricos para cada modelo son:  
\ 

```{r, echo=FALSE, out.height="10%"}

matrizTauEmpiricos<- matrix(c(none.dfA@teststat,drift.dfA@teststat[1,1],trend.dfA@teststat[1,1]), c(none.dfB@teststat,drift.dfB@teststat[1,1],trend.dfB@teststat[1,1]), nrow = 3, ncol = 2,dimnames = list(c("None","Con Drift","Con Trend"), c("Serie A", "Serie B")))
knitr::kable(matrizTauEmpiricos)

```

#### Y los respectivos valores criticos de Tau son
\ 

```{r, echo=FALSE}  
a<-c(none.dfA@cval[1,1:3])
b<-c(drift.dfA@cval[1,1:3])
c<-c(trend.dfA@cval[1,1:3])
tau_crit<-rbind(a,b,c)
rownames(tau_crit)<-c("None","Con Drift","Con Trend")

knitr::kable(tau_crit)
```
\ 

\ Para la serie A se puede afirmar con un nivel de significatividad aproximado a 5% que la serie es no estacionaria para todos los modelos. En cuanto a la serie B, existe evidencia suficiente para no rechazar la hipótesis nula de estacionariedad para modelos *none* y *drift*. Sin embargo, se rechaza para un modelo con tendencia. (Supone un $\alpha$ del 5%)    

#### Fase de estimacion

\ A continuacion, se evaluaron todas las posibles combinaciones de modelos ARIMA (p,d,q) con p, d y q entre cero cuatro y tres con el fin de seleccionar aquel que mejor ajusta en base a los criterios de informacion vistos en clase.


```{r, echo=FALSE, warning=FALSE}
coefs = c(0,1,2,3,4)

rowNames = c()

aicArraySerieA = c()
bicArraySerieA = c()

aicArraySerieB = c()
bicArraySerieB = c()

for(p in coefs){
  for(d in coefs){
    for(q in coefs){
      name = sprintf("ARIMA(%d, %d, %d)", p,d,q)

      rowNames = c(rowNames, name)
      
    
      tryCatch(
        {
          newModelSerieA = arima(SerieA, c(p,d,q), method="ML")
        },
        error=function(cond) {
          message(cond)
          
          newModelSerieA = arima(SerieA, c(p,d,q))
        },
        finally = function(a){
        }
      )
      aicArraySerieA = c(aicArraySerieA, newModelSerieA$aic)
      bicArraySerieA = c(bicArraySerieA, BIC(newModelSerieA))
      
      
      tryCatch(
        {
          newModelSerieB = arima(SerieB, c(p,d,q), method="ML")
        },
        error=function(cond) {
          message(cond)
          newModelSerieB = arima(SerieB, c(p,d,q))
        },
        finally = function(a){
        }
      )
      aicArraySerieB = c(aicArraySerieB, newModelSerieB$aic)
      bicArraySerieB = c(bicArraySerieB, BIC(newModelSerieB))
    }
  }
}

serieAInfo = data.frame(AIC=aicArraySerieA, BIC=bicArraySerieA)
serieBInfo = data.frame(AIC=aicArraySerieB, BIC=bicArraySerieB)

rownames(serieAInfo) = rowNames
rownames(serieBInfo) = rowNames

minAICserieA = min(serieAInfo$AIC)
minBICserieA = min(serieAInfo$BIC)
nameMinAICserieA = rowNames[which(serieAInfo$AIC==min(serieAInfo$AIC))]
nameMinBICserieA = rowNames[which(serieAInfo$BIC==min(serieAInfo$BIC))]

minAICserieB = min(serieBInfo$AIC)
minBICserieB = min(serieBInfo$BIC)
nameMinAICserieB = rowNames[which(serieBInfo$AIC==min(serieBInfo$AIC))]
nameMinBICserieB = rowNames[which(serieBInfo$BIC==min(serieBInfo$BIC))]


sprintf("Para la serie A, el menor AIC fue de %s para el modelo %s", minAICserieA, nameMinAICserieA)
sprintf("Para la serie A, el menor BIC fue de %s para el modelo %s", minBICserieA, nameMinBICserieA)
sprintf("Para la serie B, el menor AIC fue de %s para el modelo %s", minAICserieB, nameMinAICserieB)
sprintf("Para la serie B, el menor BIC fue de %s para el modelo %s", minBICserieB, nameMinBICserieB)
```

\ Para la Serie A, el modelo que mejor se ajusta segun el criterio BIC es un ARIMA (2,1,1).

```{r, echo=FALSE}
ModeloA<-arima(SerieA, order=c(2,1,1))
```

$$\hat{y_t}=-1.2741y_{t-1}-0.7230y_{t-2}+0.6280\epsilon_{t-1}$$
\ Para la Serie B, el que mejor se ajusta segun BIC es el modelo ARIMA (0,1,2)

```{r, echo=FALSE}
ModeloB<-arima(SerieB, order=c(0,1,2))
```

$$\hat{y_t}=1.2202\epsilon_{t-1}-0.3677\epsilon_{t-2}$$

#### Fase de validación

\ 
\ Se demuestra primero que los coeficientes del modelo estimado son significativos. Para esto es utilizada una función que devuelve un mensaje. Se ingresan los modelos estimado como input y el codigo devuelve

```{r, echo=FALSE}
TestT<-function(modelo){
  for(i in 1:length(modelo$coef)) {
      a=abs(modelo$coef[i]/sqrt(modelo$var.coef[i,i]))
      if(a>=2) {
        print(paste("El coeficiente",names(modelo$coef[i]),"es un parametro significativo"))
      } else {
        paste(print("no es significativo"))
      }
  }
}

TestT(ModeloA)
TestT(ModeloB)
```

\ El siguiente paso es que los modelos cumplan con las condiciones de estacionariedad e invertibilidad.

```{r, echo=FALSE, fig.width=5, fig.height=5, out.width = "60%", out.height="30%"}
par(mfrow=c(1,2))
plot(ModeloA)
plot(ModeloB)
```

\ Las raices caen dentro del circulo unidad. Ahora se analiza la normalidad de los residuos. Para ello se realiza un gráfico de *qqnorm* que compara los cuantiles teóricos de la distribución normal con los de los residuos de la serie.

```{r, echo=FALSE, fig.width=20, fig.height=10, out.width = "100%", out.height="100%"}
par(mfrow=c(1,2))
qqnorm(ModeloA$residuals, main="QQ-Plot Serie A")
qqline(ModeloA$residuals)
qqnorm(ModeloB$residuals, main="QQ-Plot Serie B")
qqline(ModeloB$residuals)
```

\ Se observa que los cuantiles teóricos coinciden con los cuantiles de los residuos. Para complementar el **analisis de normalidad** es realizado un test de Jarque Bera

```{r, eval=FALSE, echo=FALSE, fig.width=10, fig.height=10}
jarque.bera.test(ModeloA$residuals)
jarque.bera.test(ModeloB$residuals)
```

\  Como los *p-value* de ambas series son considerablemente mayores a un $\alpha$ de 0.1 no se rechaza la hipotesis nula de normalidad de los reisudos. Por ultimo, la **incorrelacion de los residuos** es analizada. Se ejecuta el test de Ljung-Box y se obtiene,

```{r, eval= FALSE,echo=FALSE, fig.width=10, fig.height=10}
Box.test(ModeloA$residuals,lag=4,type ="Ljung-Box",fitdf = 3)
Box.test(ModeloA$residuals,lag=6,type ="Ljung-Box",fitdf = 3)
Box.test(ModeloA$residuals,lag=8,type ="Ljung-Box",fitdf = 3)
Box.test(ModeloB$residuals,lag=4,type ="Ljung-Box",fitdf = 3)
Box.test(ModeloB$residuals,lag=6,type ="Ljung-Box",fitdf = 3)
Box.test(ModeloB$residuals,lag=8,type ="Ljung-Box",fitdf = 3)
```

\ Los test de incorrelación para rezagos de 4, 6, y 8 arrojan respectivamente valores *p* mayores a un $\alpha$ de 0.1. Por lo tanto, hay suficiente evidencia de que los residuos de ambos modelos estan incorrelacionados para valores defasados.

#### Fase de Predicción

\  Para la serie B no es correcto predecir a partir de un modelo de medias móviles. Sin embargo, se ejecuta el código para observar los resultados. La predicción finalmente para un horizonte de uno, dos, tres y veinte periodos y con un nivel de confianza al 95% es.

```{r, echo=FALSE, fig.width=25, fig.height=10, out.height="60%", out.width="90%"}
par(mfrow=c(2,2))
plot(forecast(ModeloA, 1,level=95), main="Prediccion 1 periodo A")
plot(forecast(ModeloA, 2,level=95), main="Prediccion 2 periodos A")
plot(forecast(ModeloA, 3,level=95), main="Prediccion 3 periodos A")
plot(forecast(ModeloA, 20,level=95), main="Prediccion 20 periodos A")
```

```{r, echo=FALSE, fig.width=25, fig.height=10, out.height="60%", out.width="90%"}
par(mfrow=c(2,2))
plot(forecast(ModeloB, 1,level=95), main="Prediccion 1 periodo B")
plot(forecast(ModeloB, 2,level=95), main="Prediccion 2 periodos B")
plot(forecast(ModeloB, 3,level=95), main="Prediccion 3 periodos B")
plot(forecast(ModeloB, 20,level=95), main="Prediccion 20 periodos B")
```

\ Los valores de los gráficos son representados en los siguientes cuadros. Se calcularon los intervalos para el 94%, 95% y 99% de confianza. 

```{r, echo=FALSE, out.width="50%", out.height="50%"}
argumentos <- c(1,2,3, 20)

x<-cbind(
  forecast(ModeloA, 20, level=0.99)$lower[argumentos],  
  forecast(ModeloA, 20, level=0.95)$lower[argumentos],
  forecast(ModeloA, 20, level=0.94)$lower[argumentos],
  forecast(ModeloA, 20, level=0.94)$mean[argumentos],
  forecast(ModeloA, 20, level=0.94)$upper[argumentos],
  forecast(ModeloA, 20, level=0.95)$upper[argumentos],
  forecast(ModeloA, 20, level=0.99)$upper[argumentos]
)
y<-cbind(
  forecast(ModeloB, 20, level=0.99)$lower[argumentos],  
  forecast(ModeloB, 20, level=0.95)$lower[argumentos],
  forecast(ModeloB, 20, level=0.94)$lower[argumentos],
  forecast(ModeloB, 20, level=0.94)$mean[argumentos],
  forecast(ModeloB, 20, level=0.94)$upper[argumentos],
  forecast(ModeloB, 20, level=0.95)$upper[argumentos],
  forecast(ModeloB, 20, level=0.99)$upper[argumentos]
)
texto <-c("Limite inferior 0.99","Limite inferior 0.95","Limite inferior 0.94","Prediccion","Limite sueprior 0.94","Limite superior 0.95","Limite superior 0.99")
colnames(x)<-texto
colnames(y)<-texto
rownames(x)<-argumentos
rownames(y)<-argumentos

knitr::kable(x,
   align="l",
   caption="Intervalos de confianza para el Modelo A",
) %>% kable_styling(font_size = 5, latex_options = "hold_position", repeat_header_text="Intervalos de confianza para el Modelo A", repeat_header_method = "replace")

knitr::kable(y,
   align="l",
   caption="Intervalos de confianza para el Modelo B",
) %>% kable_styling(font_size = 5, latex_options = "hold_position", repeat_header_text="Intervalos de confianza para el Modelo A", repeat_header_method = "replace")
```

\ Efectuado el analisis, se procede a exportar los datos en formato CSV

```{r, echo=FALSE}
Serie_estimadaA<-c(SerieA,forecast(ModeloA, 20, level=95)$mean)
Serie_extendidaB<-c(SerieB,forecast(ModeloB, 20, level=95)$mean)
write.csv(Serie_estimadaA,"/Users/iborovsky/Desktop/TPR/SerieEstimadaA.csv")
write.csv(Serie_extendidaB,"/Users/iborovsky/Desktop/TPR/SerieEstimadaB.csv")
```

#### Conclusiones

\  Se realizaron dos análisis, uno por cada serie.  
\    
\ Ambas series resultaron ser no estacionarias y validaron todos los supuestos vistos en la materia, para luego concluir con una predicción para 20 periodos. Se modelizó la primer serie para un ARIMA(2,1,1) y la segunda para un ARIMA(0,1,2). Se observa que el activo financiero correspondiente a la serie A tiene un comportamiento que a lo largo del tiempo tiende al cero. Por lo tanto la inversión en este activo financiero no sería la más adecuada. De todos modos, se debería tener en cuenta que la predicción para un horizonte mayor a 2 en un modelo ARIMA(2,1,1) conlleva un error significativo. 
Se observa que el activo financiero correspondiente a la serie B tiene un comportamiento que una vez superado el orden de la parte MA del modelo, es constante y aproximadamente cero.
Por lo tanto la inversión en este activo financiero no sería adecuada. De todos modos, se debería tener en cuenta que al realizar la predicción con un modelo MA se obtienen resultados muy poco certeros, con una gran carga de error. Es por ello que la decisión de invertir o no en dicho activo no puede basarse en esta predicción, se debrían buscar otros modelos que tengan unna mayor capacidad predictiva.
\  

#### Futuras investigaciones y limitaciones

\  El análisis efectuado se limitó a los modelos de la familia ARIMA. No se tomó en cuenta la posibilidad de realizar una diferencación fraccionaria, ni modelizar la serie en otras familias de modelos para series de tiempo.    
\  Por otra parte, se utilizaron los tests vistos y estudiados en la materia *751-Estadistica Actuarial*. Como se utilizó Dickey-Fuller para el análisis de raíces unitaras se pudieron haber utilizado otros tests como el de KPSS o Phillips-Perron.

#### Bibliografía

* *Analisis de series temporales: modelos ARIMA* - Ezequiel Uriel Jimenez; Valencia, España, 1985.
* *"Time Series Analysis. Forecasting and Control. Fourth edition"* - George E. P. Box, Gwilym M. Jenkins, Gregory C. Reinsel.
* *Acerca de la probabilidad* - Alberto H Landro, 2010.